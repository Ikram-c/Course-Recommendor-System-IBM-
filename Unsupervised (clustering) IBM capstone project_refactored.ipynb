{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler, Normalizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "#user profiles\n",
    "user_profile_df = pd.read_csv(\"user_profile.csv\")\n",
    "\n",
    "#Users we want to give reccomendations to\n",
    "test_users_df = pd.read_csv('rs_content_test.csv')[['user', 'item']]\n",
    "\n",
    "# Surpress any warnings:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain Feature Names and user_ids\n",
    "feature_names = user_profile_df.columns[1:]\n",
    "user_ids = user_profile_df['user']\n",
    "\n",
    "#Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the feature vectors\n",
    "user_profile_df[feature_names] = scaler.fit_transform(user_profile_df[feature_names])\n",
    "X = user_profile_df[feature_names].values\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_cluster_labels(user_ids, labels, X):\n",
    "    \"\"\"Combine user IDs with cluster labels and calculate the silhouette score.\n",
    "    \n",
    "    Args:\n",
    "        user_ids (DataFrame): A DataFrame containing the user IDs.\n",
    "        labels (list): A list of cluster labels.\n",
    "        X (array-like): The data used to generate the cluster labels.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the cluster DataFrame and silhouette score.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Combine user IDs with cluster labels\n",
    "    labels_df = pd.DataFrame(labels)\n",
    "    cluster_df = pd.merge(user_ids, labels_df, left_index=True, right_index=True)\n",
    "    cluster_df.columns = ['user', 'cluster']\n",
    "    \n",
    "    # Calculate the silhouette score\n",
    "    score = silhouette_score(X, labels)\n",
    "    \n",
    "    return cluster_df, score\n",
    "\n",
    "\n",
    "def sum_squared_distances(estimator, X):\n",
    "    \"\"\"Calculate the sum of squared distances between each point and its assigned cluster center.\n",
    "    \n",
    "    Args:\n",
    "    - estimator: The clustering estimator object.\n",
    "    - X: The feature matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - The sum of squared distances.\n",
    "    \"\"\"\n",
    "    centers = estimator.cluster_centers_\n",
    "    cluster_assignments = estimator.predict(X)\n",
    "    sum_squared_distances = sum(\n",
    "        np.sum((X[cluster_assignments == i] - centers[i]) ** 2) for i in range(estimator.n_clusters)\n",
    "    )\n",
    "    return sum_squared_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "n_clusters = results['param_n_clusters']\n",
    "mean_scores = results['mean_test_score']\n",
    "\n",
    "# Plot the mean cross-validation scores for each value of n_clusters\n",
    "plt.plot(n_clusters, mean_scores)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Mean cross-validation score')\n",
    "plt.show()\n",
    "\n",
    "## New Cell\n",
    "# Get the best number of clusters\n",
    "best_n_clusters = grid_search.best_params_['n_clusters']\n",
    "sum_squared_distances = -1 * grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "# Use the best number of clusters to fit the model\n",
    "best_kmeans = KMeans(n_clusters=15)\n",
    "best_kmeans.fit(X)  \n",
    "cluster_labels = best_kmeans.labels_\n",
    "\n",
    "\n",
    "# Obtain the clustered dataframe and the score\n",
    "cluster_df, score = combine_cluster_labels(user_ids, cluster_labels, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cluster_df for a quick visual check of the dataframe\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_item_enrol(cluster_df, test_users_df):\n",
    "    \"\"\"Group test users by cluster and count the number of enrollments for each course.\n",
    "    \n",
    "    Args:\n",
    "    cluster_df (pandas.DataFrame): A DataFrame with user and cluster columns.\n",
    "    test_users_df (pandas.DataFrame): A DataFrame with user and item columns.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the following two DataFrames:\n",
    "        - cluster_item_enrol_df (pandas.DataFrame): A DataFrame with columns cluster, item and enrollments.\n",
    "        - test_users_labelled (pandas.DataFrame): A DataFrame with columns user, item and cluster.\n",
    "    \"\"\"\n",
    "    #Merge the test_df with the cluster_df to assign cluster label to test user\n",
    "    test_users_labelled = pd.merge(test_users_df, cluster_df, left_on='user', right_on='user')\n",
    "\n",
    "    #Enrollments count for each course in each group\n",
    "    courses_cluster = test_users_labelled[['item', 'cluster']]\n",
    "    courses_cluster['count'] = [1] * len(courses_cluster)\n",
    "    cluster_item_enrol_df = courses_cluster.groupby(['cluster','item']).agg(enrollments = ('count','sum')).reset_index()\n",
    "    return cluster_item_enrol_df, test_users_labelled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_item_enrol_df, test_users_labelled = cluster_item_enrol(cluster_df, test_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def results_dict_function(cluster_item_enrol_df, test_users_df):\n",
    "    \"\"\"\n",
    "    Generate a dictionary of recommendation results for each test user, including a dataframe of all recommended courses,\n",
    "    the average number of recommendations per user, a list of the number of recommendations for each user, and a \n",
    "    dataframe of the top 10 most frequently recommended courses.\n",
    "    \n",
    "    Args:\n",
    "    - cluster_item_enrol_df (pandas.DataFrame): A dataframe containing the cluster, item, and enrollments columns\n",
    "        for each course.\n",
    "    - test_users_df (pandas.DataFrame): A dataframe containing the user and item columns for each test user.\n",
    "    \n",
    "    Returns:\n",
    "    - results_dict (dict): A dictionary containing the following keys:\n",
    "        - 'all_recommendations_df': A dataframe containing all recommended courses for all test users.\n",
    "        - 'average_num_recommendations': The average number of recommendations per test user.\n",
    "        - 'user_mean_recommendations': A list of the number of recommendations for each test user.\n",
    "        - 'top_10_courses': A dataframe of the top 10 most frequently recommended courses.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set threshold\n",
    "    enrollment_count_threshold = 10\n",
    "\n",
    "    # Filter courses with an enrollment count larger than the threshold\n",
    "    popular_courses_df = cluster_item_enrol_df[cluster_item_enrol_df['enrollments'] > enrollment_count_threshold]\n",
    "\n",
    "    # Find courses not taken by test user\n",
    "    unseen_courses = popular_courses_df[~popular_courses_df['item'].isin(test_users_labelled)]\n",
    "\n",
    "    # Group the merged DataFrame by the user column and get the list of courses taken by each test user\n",
    "    test_user_courses = pd.merge(test_users_labelled, cluster_item_enrol_df, on=['cluster', 'item']).groupby('user')['item'].apply(list).to_dict()\n",
    "\n",
    "    # Initialize a dictionary to store the recommendation results for each test user\n",
    "    recommendation_results = {}\n",
    "\n",
    "    # Loop through the test users\n",
    "    for user, cluster in test_users_labelled[['user', 'cluster']].values:\n",
    "        # Filter courses to only include those taken by users in the same cluster as the test user\n",
    "        filtered_courses_df = popular_courses_df[popular_courses_df['cluster'] == cluster]\n",
    "\n",
    "        # Get the list of courses taken by the test user or an empty list if the user is not found\n",
    "        test_user_courses_list = test_user_courses.get(user, [])\n",
    "\n",
    "        # Find courses in filtered dataframe not taken by the test user\n",
    "        recommendations = filtered_courses_df[filtered_courses_df['item'].isin(test_user_courses_list)]\n",
    "\n",
    "        # Add the recommendation results for the test user to the dictionary\n",
    "        recommendation_results[user] = recommendations\n",
    "\n",
    "    # Calculate the mean number of recommendations for each user\n",
    "    user_mean_recommendations = [len(recommendations) for recommendations in recommendation_results.values()]\n",
    "\n",
    "    # Calculate the mean of the user_mean_recommendations list\n",
    "    average_num_recommendations = np.mean(user_mean_recommendations)\n",
    "\n",
    "    # Concatenate the dataframes in the recommendation_results dictionary into a single dataframe\n",
    "    all_recommendations_df = pd.concat(recommendation_results.values())\n",
    "\n",
    "    # Get the top 10 most frequently recommended courses\n",
    "    top_10_courses = all_recommendations_df['item'].value_counts().head(10).reset_index(name='count').rename(columns={'index': 'item'})\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    results_dict = {\n",
    "        \"all_recommendations_df\": all_recommendations_df,\n",
    "        \"average_num_recommendations\": average_num_recommendations,\n",
    "        \"user_mean_recommendations\": user_mean_recommendations,\n",
    "        \"top_10_courses\": top_10_courses,\n",
    "    }\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_gridseach = results_dict_function(cluster_item_enrol_df, test_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2\n",
    "\n",
    "#cluster_df_algorithm\n",
    "#Optimizer: lowest sum of squares,     #Scaler: StandardScaler         #PCA = NO\n",
    "\n",
    "#cluster_item_enrol\n",
    "#cluster_df, test_users_df\n",
    "\n",
    "#reccomend_unseen\n",
    "#legacy_results_dict = None #hyperparms = 'grid_standard'\n",
    "\n",
    "\n",
    "# Fit k-means model to find the optimal number of clusters\n",
    "def lowest_sum_of_squares(X):\n",
    "    model = KMeans()\n",
    "    scores = []\n",
    "    for n_clusters in range(1, 25):\n",
    "        model.n_clusters = n_clusters\n",
    "        model.fit(X)\n",
    "        scores.append(-model.score(X))\n",
    "    best_n_clusters = np.argmin(scores) + 1\n",
    "    #model.n_clusters = best_n_clusters\n",
    "    model.n_clusters = 22\n",
    "    model.fit(X)\n",
    "    # Assign cluster labels to each user\n",
    "    clusters = model.predict(X)\n",
    "    cluster_labels = model.labels_\n",
    "    return cluster_labels\n",
    "\n",
    "cluster_df, score_2 = combine_cluster_labels(user_ids, cluster_labels, X)\n",
    "\n",
    "# Print cluster_df for a quick visual check of the dataframe\n",
    "cluster_df\n",
    "\n",
    "cluster_item_enrol_df, test_users_labelled = cluster_item_enrol(cluster_df, test_users_df)\n",
    "\n",
    "\n",
    "results_dict_gridseach_2 = results_dict_function(cluster_item_enrol_df, test_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3\n",
    "\n",
    "#cluster_df_algorithm\n",
    "#Optimizer: gap_statistic,     #Scaler: StandardScaler         #PCA = NO\n",
    "\n",
    "#cluster_item_enrol\n",
    "#cluster_df, test_users_df\n",
    "\n",
    "#reccomend_unseen\n",
    "#legacy_results_dict = None #hyperparms = 'grid_standard'\n",
    "\n",
    "\n",
    "\n",
    "def optimize_clusters_gap_statistic(X, n_reference_datasets=10, max_clusters=30):\n",
    "    #Random seed for reproducibility\n",
    "    np.random.seed(64)\n",
    "    # Generate reference datasets\n",
    "    reference_datasets = [np.random.permutation(X) for _ in range(n_reference_datasets)]\n",
    "\n",
    "    # Compute the gap statistic for each number of clusters\n",
    "    gap_statistics = []\n",
    "    for n_clusters in range(1, max_clusters+1):\n",
    "        model = KMeans(n_clusters=n_clusters)\n",
    "        model.fit(X)\n",
    "        Wk = model.inertia_\n",
    "        Wk_reference = [model.fit(X_reference).inertia_ for X_reference in reference_datasets]\n",
    "        gap = np.log(np.mean(Wk_reference)) - np.log(Wk)\n",
    "        gap_statistics.append(gap)\n",
    "\n",
    "    # Choose the optimal number of clusters\n",
    "    best_n_clusters = np.argmax(gap_statistics) + 1\n",
    "    return best_n_clusters\n",
    "\n",
    "# Run the Gap statistic optimizer\n",
    "#best_n_clusters = optimize_clusters_gap_statistic(X)\n",
    "best_n_clusters = optimize_clusters_gap_statistic(X, n_reference_datasets=10, max_clusters=30)\n",
    "# Fit k-means model with the optimal number of clusters\n",
    "model = KMeans(n_clusters=best_n_clusters)\n",
    "model.fit(X)\n",
    "\n",
    "# Assign cluster labels to each user\n",
    "clusters = model.predict(X)\n",
    "cluster_labels = model.labels_\n",
    "\n",
    "\n",
    "cluster_df, score_3 = combine_cluster_labels(user_ids, cluster_labels, X)\n",
    "\n",
    "# Print cluster_df for a quick visual check of the dataframe\n",
    "cluster_df\n",
    "\n",
    "cluster_item_enrol_df, test_users_labelled = cluster_item_enrol(cluster_df, test_users_df)\n",
    "\n",
    "\n",
    "results_dict_gridseach_3 = results_dict_function(cluster_item_enrol_df, test_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_names = ['model 1', 'model 2', 'model 3']\n",
    "score_list = [score, score_2, score_3]\n",
    "# create a dataframe with the values and names\n",
    "df_results_final = pd.DataFrame({'Silhoette score': score_list, 'model': df_list_names})\n",
    "\n",
    "# use seaborn to plot the bar chart\n",
    "sns.barplot(x='model', y='Silhoette score', data=df_results_final)\n",
    "plt.title('Performance of each model')\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c31c2c93585b55e010b9381178aeeb9bb3aa7fa708fada384ae012c2fbd9b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
